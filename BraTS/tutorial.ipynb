{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BraTS \n",
    "In this Notebook we will demonstrate how to use the BraTS package to use top performing algorithms from the BraTS challenges.\n",
    "\n",
    "---\n",
    "## Getting Started\n",
    "\n",
    "#### This tutorial requires:\n",
    "\n",
    "   - Python 3.8+\n",
    "   - Docker: Installation instructions on the official [website](https://docs.docker.com/get-docker/)\n",
    "\n",
    "\n",
    "#### Optional but recommended:\n",
    "   <!--CUDA 11.4+ (https://developer.nvidia.com/cuda-toolkit)-->\n",
    "   - GPU with CUDA support (*otherwise CPU can be used for a some algorithms*) \n",
    "   - NVIDIA Container Toolkit: Refer to the [NVIDIA install guide](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html) and the official [GitHub page](https://github.com/NVIDIA/nvidia-container-toolkit) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Installations\n",
    "!pip install brats matplotlib > /dev/null\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you installed the packages and requirements on your own machine, you can skip this section and start from the import section.\n",
    "\n",
    "### Setup Colab environment (optional) \n",
    "Otherwise you can follow and execute the tutorial on your browser.\n",
    "In order to start working on the notebook, click on the following button, this will open this page in the Colab environment and you will be able to execute the code on your own (*Google account required*).\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/BrainLesion/tutorials/blob/main/AURORA/tutorial.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "\n",
    "Now that you are visualizing the notebook in Colab, run the next cell to install the packages we will use. There are few things you should follow in order to properly set the notebook up:\n",
    "1. Warning: This notebook was not authored by Google. Click on 'Run anyway'.\n",
    "1. When the installation commands are done, there might be \"Restart runtime\" button at the end of the output. Please, click it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you run the next cell in a Google Colab environment, it will **clone the 'tutorials' repository** in your google drive. This will create a **new folder** called \"tutorials\" in **your Google Drive**.\n",
    "All generated file will be created/uploaded to your Google Drive respectively.\n",
    "\n",
    "After the first execution of the next cell, you might receive some warnings and notifications, please follow these instructions:\n",
    "   - 'Permit this notebook to access your Google Drive files?' Click on 'Yes', and select your account.\n",
    "   - Google Drive for desktop wants to access your Google Account. Click on 'Allow'.\n",
    "\n",
    "Afterwards the \"tutorials\" folder has been created. You can navigate it through the lefthand panel in Colab. You might also have received an email that informs you about the access on your Google Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# Check if we are in google colab currently\n",
    "try:\n",
    "    import google.colab\n",
    "\n",
    "    colabFlag = True\n",
    "except ImportError as r:\n",
    "    colabFlag = False\n",
    "\n",
    "# Execute certain steps only if we are in a colab environment\n",
    "if colabFlag:\n",
    "    # Create a folder in your Google Drive\n",
    "    from google.colab import drive\n",
    "\n",
    "    drive.mount(\"/content/drive\")\n",
    "    # clone repository and set path\n",
    "    !git clone https://github.com/BrainLesion/tutorials.git /content/drive/MyDrive/tutorials\n",
    "    BASE_PATH = \"/content/drive/MyDrive/tutorials/BraTS/\"\n",
    "    sys.path.insert(0, BASE_PATH)\n",
    "\n",
    "else:  # normal jupyter notebook environment\n",
    "    BASE_PATH = \"./\"  # current working directory would be BraTs-Toolkit anyways if you are not in colab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from brats import AdultGliomaSegmenter\n",
    "from brats.constants import AdultGliomaAlgorithms\n",
    "import utils  # local file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "AURORA expects *preprocessed* input data as NIfTI file or NumPy Array (*preprocessed* meaning the files should be co-registerend, skullstripped and in SRI-24 space).\n",
    "\n",
    "In this example we provide sample data from the [ASNR-MICCAI BraTS Brain Metastasis Challenge](https://www.synapse.org/#!Synapse:syn51156910/wiki/622553), which is already preprocessed in the `AURORA/data` folder in the form of 4 modalities of the same brain (T1, T1C, T2, FLAIR). To get an intuition of the data, one example slice of the 3D scans is visualized below.\n",
    "\n",
    "For your own data:\n",
    "If the data is *not* preprocessed yet, consider using our [BrainLes preprocessing](https://github.com/BrainLesion/preprocessing) package (or its predecessor [BraTS-Toolkit](https://github.com/neuronflow/BraTS-Toolkit)).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.visualize_data(f\"{BASE_PATH}/data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using AURORA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minimal example using default settings and only T1c as input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We first need to create an instance of the AuroraInfererConfig class,\n",
    "# which will hold the configuration for the inferer.\n",
    "# We can then create an instance of the AuroraInferer class, which will be used to perform the inference.\n",
    "\n",
    "config = AuroraInfererConfig(\n",
    "    tta=False,\n",
    "    # we disable test time augmentations for a quick demo\n",
    "    # should be set to True for better results\n",
    "    sliding_window_batch_size=4,\n",
    "    # The batch size used for the sliding window inference\n",
    "    # decrease if you run out of memory\n",
    "    # warning: too small batches might lead to unstable results\n",
    "    cuda_devices=\"0\",  # optional, if you have multiple GPUs you can specify which one to use\n",
    "    device=\"cpu\",  # uncomment this line to force-use CPU\n",
    ")\n",
    "\n",
    "\n",
    "# Now that we have the configuration we can create an instance of the AuroraInferer class.\n",
    "# This class will be used to perform the inference. We can then call the infer method to perform the inference.\n",
    "inferer = AuroraInferer(config=config)\n",
    "\n",
    "if torch.cuda.is_available() == False and colabFlag == True:\n",
    "    raise RuntimeWarning(\n",
    "        \"You are not using any GPU in Colab! Go to 'Runtime'->'Change Runtime type' to select GPU usage!\"\n",
    "    )\n",
    "\n",
    "# The infer method takes the path to the T1c MRI file and the path to the output segmentation file as arguments.\n",
    "# The output segmentation file will be created by the infer method and\n",
    "# will contain the segmentation of the input T1c MRI.\n",
    "\n",
    "# The example below shows how to perform the inference using a T1c MRI file:\n",
    "_ = inferer.infer(\n",
    "    t1c=f\"{BASE_PATH}/data/t1c.nii.gz\",\n",
    "    segmentation_file=f\"{BASE_PATH}/output/t1c_segmentation.nii.gz\",\n",
    ")\n",
    "\n",
    "# IMPORTANT: If this cell produces an OutOfMemoryError, you might not have enough VRAM (minimum 8GB).\n",
    "# Try using the CPU instead by setting \"useGPU\" to False above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize results\n",
    "\n",
    "The segementation comprise of the\n",
    "- **metastasis label** (in blue), consiting of contrast-enhancing metastasis and necrosis\n",
    "- T2-FLAIR hyperintense **edema label** (in red)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.visualize_segmentation(\n",
    "    modality_file=f\"{BASE_PATH}/data/t1c.nii.gz\",\n",
    "    segmentation_file=f\"{BASE_PATH}/output/t1c_segmentation.nii.gz\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple input modalities and other available outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AURORA also supports different combinations of multi-modal MRI files [(see manuscript)](https://www.sciencedirect.com/science/article/pii/S016781402389795X). It will automatically select a suitable model depending on the inputs supplied.\n",
    "\n",
    "- Any of the following combination of sequences can be supplied: \n",
    "    - T1-CE only\n",
    "    - T1 only\n",
    "    - T2-FLAIR only\n",
    "    - T1-CE + T2-FLAIR\n",
    "    - T1-CE + T1\n",
    "    - T1-CE + T1 + T2-FLAIR\n",
    "    - T1-CE + T1 + T2 + T2-FLAIR \n",
    "  \n",
    "- For the last combination (with all 4 sequences), the [(vanilla model)](https://www.sciencedirect.com/science/article/pii/S0167814022045625) can also be used.\n",
    "\n",
    "- Instead of only saving the final output consisting of one file with 2 labels, additional files with labels for the whole lesion (metastasis + edema) or the metastasis only can also be saved.\n",
    "\n",
    "- Test-time augmentation can be enabled (tta parameter in config, default = True). Segmentation with TTA will take around 10 times longer than without TTA.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The example below shows how to perform the inference using multi-modal inputs.\n",
    "*(This may take a while)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Instantiate the AuroraInferer\n",
    "inferer = AuroraInferer()\n",
    "\n",
    "inferer = AuroraInferer(config=config)\n",
    "\n",
    "# Use all four input modalities,we also create other outputs and a custom log file\n",
    "_ = inferer.infer(\n",
    "    t1=f\"{BASE_PATH}/data/t1.nii.gz\",\n",
    "    t1c=f\"{BASE_PATH}/data/t1c.nii.gz\",\n",
    "    t2=f\"{BASE_PATH}/data/t2.nii.gz\",\n",
    "    fla=f\"{BASE_PATH}/data/flair.nii.gz\",\n",
    "    segmentation_file=f\"{BASE_PATH}/output/multi-modal_segmentation.nii.gz\",\n",
    "    # The unbinarized network outputs for the whole tumor channel (edema + enhancing tumor core + necrosis) channel\n",
    "    whole_tumor_unbinarized_floats_file=f\"{BASE_PATH}/output/whole_tumor_unbinarized_floats.nii.gz\",\n",
    "    # The unbinarized network outputs for the metastasis (tumor core) channel\n",
    "    metastasis_unbinarized_floats_file=f\"{BASE_PATH}/output/metastasis_unbinarized_floats.nii.gz\",\n",
    "    log_file=f\"{BASE_PATH}/output/custom_logfile.log\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
